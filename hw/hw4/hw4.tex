
\documentclass[letterpaper, reqno,11pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{color,latexsym,amsmath,amssymb}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage[linesnumbered,lined,boxed,commentsnumbered,noend,noline]{algorithm2e}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{framed}
\usepackage{titling}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}

\tikzset{invclip/.style={clip,insert path={{[reset cm]
  (-16383.99999pt,-16383.99999pt) rectangle (16383.99999pt,16383.99999pt)}}}}

\allowdisplaybreaks
\DontPrintSemicolon
\SetKw{Break}{break}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\PP}{\mathop{{}\mathbb{P}}}
\newcommand{\EE}{\mathop{{}\mathbb{E}}}
\newcommand{\inci}{\mathds{1}}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\charcone}{char.cone}
\DeclareMathOperator{\STAB}{STAB}
\DeclareMathOperator{\Down}{Down}
\DeclareMathOperator{\lca}{lca}
\DeclareMathOperator{\ex}{ex}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\comp}{c}
\DeclareMathOperator{\bernoulli}{Bernoulli}
\DeclareMathOperator{\Mod}{mod}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\LowestOne}{\textsc{LowestOne}}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\error}{error}
\SetKwFor{RepTimes}{repeat}{times}{end}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\begin{document}
\pagenumbering{arabic}
\title{Homework 4}
\author{Yuchong Pan}
\date{\today}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{exercise}{Exercise}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{solution}{Solution}
%\maketitle
%

\begin{framed}
\noindent{\bf 6.842 Randomness and Computation} \hfill \thedate
\begin{center}
\Large{\thetitle}
\end{center}
%\noindent{\em Lecturer: Ronitt Rubinfield} \hfill {\em Scribe: \theauthor}
\noindent{\em Yuchong Pan} \hfill {\em MIT ID: 911346847}
\end{framed}

\begin{enumerate}
  \item \noindent\emph{Collaborators and sources:} Guanghao Ye, Zixuan Xu.
  
  \begin{proof}
    Note that if $n \in \{ 1, 2 \}$, then the statement holds trivially. Therefore, WLOG assume that $n \geq 2$. Let $L_0$ be the set of the left vertices in the graph. For each $L \subset L_0$, let $N_1(L)$, $N_2(L)$ and $N_3(L)$ be the neighborhoods of $L$ in the subgraphs of the graph induced by the three random permutations, respectively. Then
    \begin{align}
      &\quad\, \PP\left[\exists L \subset L_0, |L| \leq \frac{n}{2}, |N(L)| < (1 + \varepsilon) |L|\right] \nonumber \\
      &\leq \sum_{\substack{L \subset L_0 \\ 0 < |L| \leq \frac{n}{2}}} \PP[|N(L)| < (1 + \varepsilon) |L|] && \text{(union bound)} \nonumber \\
      &= \sum_{\substack{L \subset L_0 \\ 0 < |L| \leq \frac{n}{2}}} \PP\left[\exists R \subset R_0 \setminus N_1(L), |R| = \lceil \varepsilon |L| \rceil  - 1, N_2(L) \cup N_3(L) \subset R\right] \nonumber \\
      &\leq \sum_{\substack{L \subset L_0 \\ 0 < |L| \leq \frac{n}{2}}} \sum_{\substack{R \subset R_0 \setminus N_1(L) \\ |R| = \lceil \varepsilon |L| \rceil - 1}} \PP\left[N_2(L) \cup N_3(L) \subset R\right] && \text{(union bound)} \nonumber \\
      &= \sum_{\ell = 1}^{\frac{n}{2}} \binom{n}{\ell} \binom{n - \ell}{\lceil \varepsilon \ell \rceil  - 1} \left(\frac{\binom{\ell + \lceil \varepsilon \ell \rceil  - 1}{\ell}}{\binom{n}{\ell}}\right)^2 \nonumber \\
      &\leq \sum_{\ell = 1}^{\frac{n}{2}} \binom{n}{\ell} \binom{n - \ell}{\lfloor \varepsilon \ell \rfloor} \left(\frac{\binom{\ell + \lfloor \varepsilon \ell \rfloor}{\ell}}{\binom{n}{\ell}}\right)^2 && \text{(for $\varepsilon \leq 1/2$)} \label{eq:binomial} \\
      &= \sum_{\ell = 1}^{\frac{n}{2}} \frac{n!}{\ell!(n - \ell)!} \cdot \frac{(n - \ell)!}{(\lfloor \varepsilon \ell \rfloor)!(n - \ell - \lfloor \varepsilon \ell \rfloor)!} \cdot \left(\frac{\frac{(\ell + \lfloor \varepsilon \ell \rfloor)!}{\ell!\lfloor\varepsilon\ell\rfloor!}}{\frac{n!}{\ell!(n - \ell)!}}\right)^2 \nonumber \\
      &= \sum_{\ell = 1}^{\frac{n}{2}} \frac{((n - \ell)!)^2((\ell + \lfloor \varepsilon \ell \rfloor)!)^2}{n!\ell!(\lfloor\varepsilon\ell\rfloor!)^3 (n - \ell - \lfloor \varepsilon\ell \rfloor)!}. \label{eq:factorials}
    \end{align}
    Stirling's approximation says that for all $k \in \NN$,
    \begin{equation} \label{eq:stirling}
      \sqrt{2\pi k} \left(\frac{k}{e}\right)^k e^{\frac{1}{12k + 1}} < k! < \sqrt{2\pi k} \left(\frac{k}{e}\right)^k e^{\frac{1}{12k}}.
    \end{equation}
    We apply \eqref{eq:stirling} to each factorial in \eqref{eq:factorials}; specifically, we apply the upper bound to the numerator and the lower bound to the denominator. First, we count the contributions of $e^k$ from the factorials in each term of \eqref{eq:factorials}:
    $$ \exp\left(n + \ell + 3\lfloor \varepsilon \ell \rfloor + (n - \ell - \lfloor \varepsilon \ell \rfloor) - (2(n - \ell) + 2(\ell + \lfloor \varepsilon \ell \rfloor))\right) = \exp(0) = 1. $$
    Second, we count the contributions of $e^{\frac{1}{12k}}$ in the upper bound and $e^{\frac{1}{12k + 1}}$ in the lower bound from the factorials in each term of \eqref{eq:factorials}:
    \begin{multline*}
      \exp\left(\frac{1}{12}\left(\frac{2}{n - \ell} + \frac{1}{\ell + \lfloor \varepsilon \ell \rfloor}\right) - \left(\frac{1}{12n + 1} + \frac{1}{12 \ell + 1} + \frac{3}{12\lfloor \varepsilon \ell \rfloor + 1} + \frac{1}{12(n - \ell - \lfloor \varepsilon \ell \rfloor) + 1}\right)\right) \\
      \leq \exp\left(\frac{1}{12} \cdot \left(2 + 1\right)\right) < 1.29,
    \end{multline*}
    by noting that $n - \ell \geq 1$ and $\ell + \lfloor \varepsilon \ell \rfloor \geq 1$ for all $\ell \in [n/2]$. Take $\varepsilon = 0.01$. Third, we count the contributions of $k^k$ from the factorials in each term of \eqref{eq:factorials}:
    \begin{align}
      &\quad\, \frac{(n - \ell)^{2(n - \ell)} (\ell + \lfloor \varepsilon \ell \rfloor)^{2(\ell + \lfloor \varepsilon \ell \rfloor)}}{n^n \ell^\ell \lfloor\varepsilon \ell\rfloor^{3\lfloor\varepsilon \ell\rfloor} (n - \ell - \lfloor \varepsilon \ell \rfloor)^{n - \ell - \lfloor \varepsilon \ell \rfloor}} \nonumber \\
      &\leq \frac{(n - \ell)^{2(n - \ell)} (\ell + \varepsilon \ell)^{2(\ell + \varepsilon \ell)}}{n^n \ell^\ell (\varepsilon \ell)^{3\varepsilon \ell} (n - \ell - \varepsilon \ell)^{n - \ell - \varepsilon \ell}} \label{eq:no-floor} \\
      &\leq \frac{(1 + \varepsilon)^{2(1 + \varepsilon)}}{\varepsilon^{3\varepsilon}} \left(\frac{n - \ell}{n}\right)^{n - \ell + \lfloor \varepsilon \ell \rfloor} \left(\frac{\ell}{n}\right)^{\ell - \lfloor \varepsilon \ell \rfloor} \left(1 + \frac{\lfloor \varepsilon \ell \rfloor}{n - \ell - \lfloor \varepsilon \ell \rfloor}\right)^{n - \ell - \lfloor \varepsilon \ell \rfloor} \nonumber \\
      &< 1.18 \cdot 1 \cdot \left(\frac{1}{2}\right)^{0.99\ell} \cdot e^{0.01 \ell} && \text{(since $\ell \leq n/2$)} \nonumber \\
      &< 1.18 \cdot 0.51^\ell. \nonumber
    \end{align}
    Note that \eqref{eq:no-floor} is due to the fact that for each term in \eqref{eq:binomial},
    $$ \binom{n}{\ell} \binom{n - \ell}{\lfloor \varepsilon \ell \rfloor} \left(\frac{\binom{\ell + \lfloor \varepsilon \ell \rfloor}{\ell}}{\binom{n}{\ell}}\right)^2 \leq \binom{n}{\ell} \binom{n - \ell}{\varepsilon \ell} \left(\frac{\binom{\ell + \varepsilon \ell}{\ell}}{\binom{n}{\ell}}\right)^2, $$
    by the monotonicity of the generalized binomial coefficient (with factorials replaced by the gamma function $\Gamma(r) = (r - 1)!$). Fourth, we count the contributions of $\sqrt{2\pi k}$ from the factorials in each term of \eqref{eq:factorials}. Note that $k^k > \sqrt{2\pi k}$ for all $k \geq 1.99$. Since $n > 3$, then $n - \ell \geq n - (1 + \varepsilon) \ell \geq 1.99$ for all $\ell \in [n/2]$. Note also that $(1 + \varepsilon) \ell \geq 2$ for all $\ell \in \{ 2, \ldots, n/2 \}$. Hence, the contribution of $\sqrt{2\pi k}$ is not upper bounded by $k^k$ from the factorials in each term only if $\ell = 1$, in which case, since $n \geq 3$,
    $$ \binom{n}{\ell} \binom{n - \ell}{\lfloor \varepsilon \ell \rfloor} \left(\frac{\binom{\ell + \lfloor \varepsilon \ell \rfloor}{\ell}}{\binom{n}{\ell}}\right)^2 = \binom{n}{1} \binom{n - 1}{\lfloor 0.01 \cdot 1 \rfloor} \left(\frac{\binom{1 + \lfloor 0.01 \cdot 1 \rfloor}{1}}{\binom{n}{1}}\right)^2 = \frac{1}{n} \leq \frac{1}{3}. $$
    Moreover, the contribution of $\sqrt{2\pi k}$ from each term with $\ell \in \{ 2, \ldots, n/2 \}$ is at most $1.18 \cdot 0.51^\ell$. Therefore,
    \begin{align*}
      &\quad\, \PP\left[\exists L \subset L_0, |L| \leq \frac{n}{2}, |N(L)| < (1 + \varepsilon) |L|\right] \\
      &\leq \sum_{\ell = 1}^{\frac{n}{2}} \binom{n}{\ell} \binom{n - \ell}{\lfloor \varepsilon \ell \rfloor} \left(\frac{\binom{\ell + \lfloor \varepsilon \ell \rfloor}{\ell}}{\binom{n}{\ell}}\right)^2 \\
      &= \binom{n}{1} \binom{n - 1}{\lfloor 0.01 \cdot 1 \rfloor} \left(\frac{\binom{1 + \lfloor 0.01 \cdot 1 \rfloor}{1}}{\binom{n}{1}}\right)^2 + \sum_{\ell = 2}^{\frac{n}{2}} \binom{n}{\ell} \binom{n - \ell}{\lfloor \varepsilon \ell \rfloor} \left(\frac{\binom{\ell + \lfloor \varepsilon \ell \rfloor}{\ell}}{\binom{n}{\ell}}\right)^2 \\
      &\leq \frac{1}{3} + \sum_{\ell = 2}^{\frac{n}{2}} 1 \cdot 1.29 \cdot \left(1.18 \cdot 0.51^{\ell}\right)^2 \\
      &< 0.34 + 1.8\sum_{\ell = 2}^{\frac{n}{2}} 0.27^\ell \\
      &< 0.34 + 1.8\sum_{\ell = 2}^\infty 0.27^\ell \\
      &= 0.34 + 1.8 \cdot 0.27^2 \cdot \frac{1}{1 - 0.27} \\
      &< 0.52.
    \end{align*}
    It follows that
    \begin{align*}
      \PP\left[|N(L)| \geq (1 + \varepsilon) |L| \;\forall L \subset L_0, |L| \leq \frac{n}{2}\right] &\geq 1 - \PP\left[\exists L \subset L_0, |L| \leq \frac{n}{2}, |N(L)| < (1 + \varepsilon) |L|\right] \\
      &> 1 - 0.52 = 0.48.
    \end{align*}
    This completes the proof.
  \end{proof}

  \clearpage

  \item \begin{enumerate}
    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Note that $\mathds 1_\text{test accepts} = (1 + f(x)f(y)f(z))/2$. By the Fourier transform of $f$ and by linearity of expecation,
      \begin{align*}
        \EE[f(x) f(y) f(z)] &= \EE\left[\left(\sum_{S \subset [n]} \hat{f}(S) \chi_S(x)\right) \left(\sum_{T \subset [n]} \hat{f}(T) \chi_T(y)\right) \left(\sum_{U \subset [n]} \hat{f}(U) \chi_U(z)\right)\right] \\
        &= \sum_{S, T, U \subset [n]} \hat{f}(S) \hat{f}(T) \hat{f}(U) \EE\left[\chi_S(x) \chi_T(y) \chi_U(x \circ y \circ w)\right].
      \end{align*}
      Let $S, T, U \subset [n]$. For all $i \in [n]$, since $x_i, y_i \in \{ \pm 1 \}$, then $x_i^2 = y_i^2 = 1$. Hence,
      \begin{align*}
        \chi_S(x) \chi_T(y) \chi_U(x \circ y \circ w) &= \left(\prod_{i \in S} x_i\right) \left(\prod_{i \in T} y_i\right) \left(\prod_{i \in U} x_i y_i w_i\right) \\
        &= \left(\prod_{i \in S \cap U} x_i^2\right) \left(\prod_{i \in T \cap U} y_i^2\right) \left(\prod_{i \in S \triangle U} x_i\right) \left(\prod_{i \in T \triangle U} y_i\right) \left(\prod_{i \in U} w_i\right) \\
        &= \chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w).
      \end{align*}
      If $S = T = U$, since $w_1, \ldots, w_n$ are all chosen independently and since $\EE[w_i] = (-1) \cdot \delta + 1 \cdot (1 - \delta) = 1 - 2\delta$ for all $i \in [m]$, then
      \begin{align*}
        \EE\left[\chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w)\right] &= \EE\left[\prod_{i \in S} w_i\right] = \prod_{i \in S} \EE\left[w_i\right] = (1 - 2\delta)^{|S|}.
      \end{align*}
      Now, suppose that either $S \neq U$ or $T \neq U$. WLOG assume that $S \neq U$. Then $S \triangle U \neq \emptyset$. Let $j \in S \triangle U$. For $x \in \{ \pm 1 \}^n$, let $x^{\oplus j}$ be the vector obtained by flipping the $j^\text{th}$ bit in $x$. Then we can partition $\{ \pm 1 \}^n$ into (unordered) pairs $(x, x^{\oplus j})$. Therefore,
      \begin{align*}
        \EE\left[\chi_{S \triangle U}(x)\right] &= \frac{1}{2^n} \sum_{x \in \{ \pm 1 \}^n} \chi_{S \triangle U}(x) = \frac{1}{2^n} \sum_{\text{pairs $\left(x, x^{\oplus j}\right)$}} \left(\chi_{S \triangle U}(x) + \chi_{S \triangle U}\left(x^{\oplus j}\right)\right) \\
        &= \frac{1}{2^n} \sum_{\text{pairs $\left(x, x^{\oplus j}\right)$}} \left(x_j \prod_{i \in (S \triangle U) \setminus \{ j \}} x_i + \left(-x_j\right) \prod_{i \in (S \triangle U) \setminus \{ j \}} x_i\right) = 0.
      \end{align*}
      Since $x$, $y$ and $w$ are chosen independently, then for all $S, T, U \subset [n]$ such that either $S \neq U$ or $T \neq U$,
      $$ \EE\left[\chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w)\right] = \EE\left[\chi_{S \triangle U}(x)\right] \EE\left[\chi_{T \triangle U}(y)\right] \EE\left[\chi_U(w)\right] = 0. $$
      Therefore,
      \begin{align*}
        \PP[\text{test accepts}] &= \EE\left[\mathds 1_\text{test accepts}\right] = \EE\left[\frac{1 + f(x)f(y)f(z)}{2}\right] = \frac{1}{2} + \frac{1}{2} \EE[f(x)f(y)f(z)] \\
        &= \frac{1}{2} + \frac{1}{2} \sum_{S, T, U \subset [n]} \hat{f}(S) \hat{f}(T) \hat{f}(U) \EE\left[\chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w)\right] \\
        &= \frac{1}{2} + \frac{1}{2} \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3.
      \end{align*}
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be a dictator function. Then $f = \chi_{\{ j \}}$ for some $j \in [n]$. Therefore, $\hat{f}(\{ j \}) = 1$ and $\hat{f}(S) = 0$ for all $S \subset [n]$ with $S \neq \{ j \}$. By part (a),
      \begin{align*}
        \PP[\text{test accepts}] &= \frac{1}{2} + \frac{1}{2} \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3 \\
        &= \frac{1}{2} + \frac{1}{2} \left((1 - 2\delta)^{|\{ j \}|} \hat{f}(\{ j \})^3 + \sum_{\substack{S \subset [n] \\ S \neq \{ j \}}} (1 - 2\delta)^{|S|} \hat{f}(S)^3\right) \\
        &= \frac{1}{2} + \frac{1}{2} \left((1 - 2\delta)^1 \cdot 1^3 + \sum_{\substack{S \subset [n] \\ S \neq \{ j \}}} (1 - 2\delta)^{|S|} \cdot 0^3\right) \\
        &= \frac{1}{2} + \frac{1}{2} (1 - 2\delta) = 1 - \delta.
      \end{align*}
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be such that $f$ passes with probability at least $1 - \varepsilon$ for some $\varepsilon \in (0, 1/2)$. By part (a),
      $$ 1 - \varepsilon \leq \PP[\text{test accepts}] = \frac{1}{2} + \frac{1}{2} \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3. $$
      Rearranging the above inequality and applying Parseval's identity yield
      \begin{align*}
        1 - 2\varepsilon &\leq \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3 \leq \left(\max_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)\right) \sum_{S \subset [n]} \hat{f}(S)^2 \\
        &= \left(\max_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)\right) \cdot 1 = \max_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S).
      \end{align*}
      Hence, there exists $S \subset [n]$ such that $(1 - 2\delta)^{|S|} \hat{f}(S) \geq 1 - 2\varepsilon$. Set $\delta = \varepsilon$ in the test. Then $(1 - 2\varepsilon)^{|S|} \hat{f}(S) \geq 1 - 2\varepsilon$. Since $\varepsilon \in (0, 1/2)$, then $1 - 2\varepsilon \in (0, 1)$, so $(1 - 2\varepsilon)^{|S|} \in (0, 1]$. Therefore,
      $$ \hat{f}(S) \geq \frac{1 - 2\varepsilon}{(1 - 2\varepsilon)^{|S|}} \geq \frac{1 - 2\varepsilon}{1} = 1 - 2\varepsilon. $$
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} none.
    
    By part (c), if $f$ passes with probability at least $1 - \varepsilon$ for some $\varepsilon \in (0, 1/2)$, then there exists $S \subset [n]$ such that $(1 - 2\varepsilon)^{|S|} \hat{f}(S) \geq 1 - 2\varepsilon$ by setting $\delta = \varepsilon$ in the test. Since $\dist(f, \chi_S) \in [0, 1]$, then $\hat{f}(S) = 1 - 2\dist(f, \chi_S) \in [-1, 1]$. Since $\varepsilon \in (0, 1/2)$, then $1 - 2\varepsilon \in (0, 1)$. If $|S| \geq 2$, then $0 < (1 - 2\varepsilon)^{|S|} < 1 - 2\varepsilon$, so $(1 - 2\varepsilon)^{|S|} \hat{f}(S) < 1 - 2\varepsilon$, a contradiction. Therefore, one of the following two cases holds:
    \begin{enumerate}[label=(\roman*), itemsep=0pt]
      \item $|S| = 1$ and $\hat{f}(S) = 1$ (so $\dist(f, \chi_S) = 0$, and $f = \chi_S$ is a dictator function);
      \item $|S| = 0$ and $\hat{f}(S) \geq 1 - 2\varepsilon$ (so $\dist(f, \chi_\emptyset) \leq \varepsilon$).
    \end{enumerate}
    Hence, if $f$ is $\varepsilon$-close to $\chi_\emptyset \equiv 1$ (a non-dictator function), then $f$ also passes with probability at least $1 - \varepsilon$.

    Note that for any dictator function, say $\chi_{\{ j \}}$ for some $j \in [n]$,
    $$ \PP_{x \in \{ \pm 1 \}^n}\left[\chi_{\{ j \}}(x) = 0\right] = \PP_{x \in \{ \pm 1 \}^n}\left[x_j = 0\right] = \frac{\left|\left\{ x \in \{ \pm 1 \}^n : x_j = 0 \right\}\right|}{2^n} = \frac{2^{n - 1}}{2^n} = \frac{1}{2}. $$
    In other words, any dictator function equals $0$ for half of the inputs, and $1$ for the other half. We give a simple fix to the test by applying the following new test before the original test. For any sufficiently small $\eta > 0$, we independently and uniformly sample $\Theta(\log (1/\eta))$ random inputs from $\{ \pm 1 \}^n$, and reject if and only if more than $3/4$ of the values are $1$. If $f$ is $\varepsilon$-close to $\chi_\emptyset \equiv 1$ for some $\varepsilon \in (0, 1/8)$, then by the Chernoff bound,
    $$ \PP[\text{new test rejects $f$}] = 1 - \PP[\text{$\leq 3/4$ of the values are $1$}] \geq 1 - e^{-\Theta(\log (1/\eta))} = 1 - \Theta(\eta). $$
    On the other hand, if $f$ is a dictator function, then by the Chernoff bound,
    $$ \PP[\text{new test accepts $f$}] = 1 - \PP[\text{$>3/4$ of the values are $1$}] \geq 1 - e^{-\Theta(\log(1/\eta))} = 1 - \Theta(\eta). $$
    Hence, if $f$ passes the combination of the new test and the original test with probability at least $1 - \varepsilon$ and with $\delta = \varepsilon$ in the original test for some sufficiently small $\varepsilon > 0$, then $f$ is a dictator function with probability at least $1 - \Theta(\eta)$; on the other hand, if $f$ is a dictator function, then the union bound implies that $f$ passes the combined test with probability at least $1 - \Theta(\eta) - \delta$. This shows that the combined test is a dictator test.
  \end{enumerate}
  
  \clearpage

  \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
  
  \begin{proof}
    Let $\mathcal A$ be a PAC learning algorithm for a class $C$ that runs in $\poly(\log n, 1/\varepsilon, 1/\delta)$ time. We denote by $\error_{\mathcal D}(h) = \PP_{x \sim \mathcal D}[h(x) \neq f(x)]$ the error of a hypothesis $h$ with respect to $f$ with inputs drawn from distribution $\mathcal D$. We denote by $\error_S(h) = |\{ x \in S : h(x) \neq f(x) \}|/|S|$ the error of $h$ in the sample set $S$. We give a PAC learning algorithm in Algorithm \ref{alg:confidence} with running time $\poly(\log n, 1/\varepsilon, \log(1/\delta))$. Let $\mathcal D$ be the distribution of inputs.

    \begin{algorithm}
      $\ell \leftarrow \lceil\log_2 (3/\delta) \rceil$ \\
      \ForEach{$i \leftarrow 1, \ldots, \ell$}{
        run $\mathcal A$ with accuracy $\varepsilon/2$ and confidence $1/2$, obtaining a hypothesis $h_i$
      }
      $m \leftarrow \lceil (12/\varepsilon^2) \log (6\ell/\delta) \rceil$ \\
      \ForEach{$j \leftarrow 1, \ldots, m$}{
        draw $x_j \sim \mathcal D$
      }
      $S \leftarrow \{ x_1, \ldots, x_m \}$ \\
      $i^* \leftarrow \argmin_{i \in [\ell]}(\error_S(h_i))$ \\
      \Return{$h_{i^*}$}
      \caption{A PAC learning algorithm with running time $\poly(\log n, 1/\varepsilon, \log(1/\delta))$, given accuracy $\varepsilon > 0$ and confidence $\delta > 0$.}
      \label{alg:confidence}
    \end{algorithm}

    Since each call to $\mathcal A$ runs in $\poly(\log n, 1/\varepsilon)$ time, then the running time of Algorithm \ref{alg:confidence} is
    $$ O\left(\log \frac{1}{\delta}\right) \poly\left(\log n, \frac{1}{\varepsilon}\right) + O\left(\frac{1}{\varepsilon^2} \log \frac{\log \frac{1}{\delta}}{\delta}\right) = \poly\left(\log n, \frac{1}{\varepsilon}, \log \frac{1}{\delta}\right). $$

    First, since $\PP[\error_{\mathcal D}(h_i) \leq \varepsilon/2] \geq 1 - 1/2 = 1/2$ for each $i \in [\ell]$, then
    $$ \PP\left[\exists i \in [\ell], \error_{\mathcal D}(h_i) \leq \varepsilon/2\right] \geq 1 - \left(1 - \frac{1}{2}\right)^\ell = 1 - \left(\frac{1}{2}\right)^{\lceil\log_2 \left(\frac{3}{\delta}\right) \rceil} \geq 1 - \left(\frac{1}{2}\right)^{\log_2 \left(\frac{3}{\delta}\right)} = 1 - \frac{\delta}{3}. $$
    Second,
    \begin{align*}
      &\quad\, \PP\left[\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| < \frac{\varepsilon}{4} \; \forall i \in [\ell]\right] \\
      &= 1 - \PP\left[\exists i \in [\ell], \left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| \geq \frac{\varepsilon}{4}\right] \\
      &\geq 1 - \sum_{i = 1}^\ell \PP\left[\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| \geq \frac{\varepsilon}{4}\right] && \text{(union bound)} \\
      &= 1 - \sum_{i = 1}^\ell \PP\left[\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| \geq \frac{\varepsilon}{4\error_{\mathcal D}\left(h_i\right)} \cdot \error_{\mathcal D}\left(h_i\right)\right] \\
      &\geq 1 - \sum_{i = 1}^\ell 2\exp\left(-\frac{1}{3} m\error_{\mathcal D}\left(h_i\right) \cdot \left(\frac{\varepsilon}{4\error_{\mathcal D}\left(h_i\right)}\right)^2\right) && \text{(Chernoff bound)} \\
      &= 1 - \sum_{i = 1}^\ell 2\exp\left(-\frac{m\varepsilon^2}{12 \error_{\mathcal D}\left(h_i\right)}\right) \\
      &\geq 1 - \ell \cdot 2\exp\left(-\frac{\left\lceil\frac{12}{\varepsilon^2} \log \frac{6\ell}{\delta}\right\rceil \cdot \varepsilon^2}{12 \cdot 1}\right) \\
      &\geq 1 - 2\ell\exp\left(-\frac{\frac{12}{\varepsilon^2} \log \frac{6\ell}{\delta} \cdot \varepsilon^2}{12}\right) \\
      &= 1 - 2\ell\exp\left(-\log \frac{6\ell}{\delta}\right) \\
      &= 1 - 2\ell \cdot \frac{\delta}{6\ell} \\
      &= 1 - \frac{\delta}{3}.
    \end{align*}
    Since $i^*$ minimizes $\error_S(h_i)$ over $i \in [\ell]$, then by the union bound,
    \begin{align*}
      \PP\left[\error_S\left(h_{i^*}\right) < \frac{3\varepsilon}{4}\right] &\geq \PP\left[\exists i \in [\ell], \error_S\left(h_i\right) < \frac{3\varepsilon}{4}\right] \\
      &\geq \PP\left[\left(\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| < \frac{\varepsilon}{4} \;\forall i \in [\ell]\right) \wedge \left(\exists i \in [\ell], \error_{\mathcal D}\left(h_i\right) \leq \frac{\varepsilon}{2}\right)\right] \\
      &\geq 1 - \left(\frac{\delta}{3} + \frac{\delta}{3}\right) \\
      &= 1 - \frac{2\delta}{3}.
    \end{align*}
    By the union bound again,
    \begin{align*}
      \PP\left[\error_{\mathcal D}\left(h_{i^*}\right) < \varepsilon\right] &\geq \PP\left[\left(\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| < \frac{\varepsilon}{4} \;\forall i \in [\ell]\right) \wedge \left(\error_S\left(h_{i^*}\right) < \frac{3\varepsilon}{4}\right)\right] \\
      &\geq 1 - \left(\frac{\delta}{3} + \frac{2\delta}{3}\right) \\
      &= 1 - \delta.
    \end{align*}
    This completes the proof.
  \end{proof}

  \clearpage

  \item \begin{enumerate}
    \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
    
    \bigskip

    First, we give an algorithm in Algorithm \ref{alg:4a} which, given a set $S$ of samples, finds a consistent decision list $h$ such that $h(x) = f(x)$ for all $x \in S$.

    \begin{algorithm}
      $h \leftarrow \text{empty decision list}$ \\
      \Repeat{$S \neq \emptyset$}{
        \ForEach{literal $\ell$ (i.e., a variable or its negation)}{
          $S' \leftarrow \{ x \in S : \ell(x) = f(x) \}$ \\
          \If{there exists $b \in \{ 0, 1 \}$ such that $f(x) = b$ for all $x \in S'$}{
            append to $h$ a decision ``if $\ell(x)$ then output $b$'' \\
            $S \leftarrow S \setminus S'$ \\
            \Break
          }
        }
      }
      \Return{$h$}
      \caption{An algorithm which, given a set $S$ of samples, finds a consistent decision list $h$ such that $h(x) = f(x)$ for all $x \in S$.}
      \label{alg:4a}
    \end{algorithm}

    We show that Algorithm \ref{alg:4a} is correct. Consider an iteration of the {\bf repeat} loop, with set $S_0$ of remaining samples. Then $S_0 \neq \emptyset$. It suffices to show that there exist a literal $\ell$ and $b \in \{ 0, 1 \}$ such that $S' \neq \emptyset$ and $f(x) = b$ for all $x \in S'$, where $S' = \{ x \in S_0 : \ell(x) = f(x) \}$. Since $f$ is a decision list, let ``if $\ell^*(x)$ then output $b^*$'' be the first decision in $f$ that has not been added to $h$ so far. Let $S_0^*$ be the set of samples that are not output by decisions prior to $\ell^*$ in $f$. Let $S^*$ be the set of samples output by decision ``if $\ell^*(x)$ then output $b^*$'' in $f$. Then $S^* = \{ x \in S_0^* : \ell^*(x) = f(x) \}$ and $f(x) = b^*$ for all $x \in S^*$. Let $S' = \{ x \in S_0 : \ell^*(x) = f(x) \}$. Since all decisions prior to $\ell^*$ in $f$ has been added to $h$, then $S_0 \subset S_0^*$, so $S' \subset S^*$. Therefore, $f(x) = b^*$ for all $x \in S'$. If $S' \neq \emptyset$, then we are done. Otherwise, we run the same argument for the next decision in $f$, until we find a decision in $f$ for which $S' \neq \emptyset$. We claim that this is possible. To see this, since $\emptyset \neq S_0 \subset S_0^*$, then there exists a decision after literal $\ell^*$ in $f$ for which $S' \neq \emptyset$. This completes the proof.

    Second, we show that $|S| = \ln(n! 4^n)$ is necessary to ensure $(\varepsilon, \delta)$ PAC learning. To see this, we claim that each variable appears at most once in a decision list. Suppose that variable $x_i$ first appears in a decision list $f$ as literal $\ell$. Let $S_0$ be the set of points $\{ 0, 1 \}^n$ that remain after this decision. Then $\ell(x) \neq f(x)$ for all $x \in S_0$. This shows that literal $\ell$ need not appear in $h$ again. Moreover, $\neg \ell(x) = f(x)$ for all $x \in S_0$. At any point after literal $\ell$ in $f$, if we were to add literal $\neg \ell$, then there would exist $b \in \{ 0, 1 \}$ such that $f(x) = b$ for all remaining $x$ such that $\neg f(x) = f(x)$; since $\neg \ell(x) = f(x)$ for all remaining $x$, then we can simply end the decision list. Therefore, if $|S| < \ln(n!4^n)$, then we can possibly output wrong answers for $2^{n-1}$ points.
  \end{enumerate}

  \clearpage

  \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
  
  \begin{proof}
    We apply Occam's Razor; i.e., we give Algorithm \ref{alg:4b}. 

    \begin{algorithm}
      draw $M = (1/\varepsilon) (\ln|\mathcal C| + \ln(1/\delta))$ samples from $\{ 0, 1 \}^n$, where $\mathcal C$ is the set of all decision lists \\
      run Algorithm \ref{alg:4a} to find a consistent decision list $h$ such that $h(x) = f(x)$ for all $x \in S$ \\
      \Return{$h$}
      \caption{An algorithm which finds a decision list $h$ such that $\PP_{x \sim \mathcal D}[f(x) \neq h(x)] < \varepsilon$ with probability at least $1 - \delta$.}
      \label{alg:4b}
    \end{algorithm}

    First, we claim that each variable appears at most once in a decision list. Suppose that variable $x_i$ first appears in a decision list $f$ as literal $\ell$. Let $S_0$ be the set of points $\{ 0, 1 \}^n$ that remain after this decision. Then $\ell(x) \neq f(x)$ for all $x \in S_0$. This shows that literal $\ell$ need not appear in $h$ again. Moreover, $\neg \ell(x) = f(x)$ for all $x \in S_0$. At any point after literal $\ell$ in $f$, if we were to add literal $\neg \ell$, then there would exist $b \in \{ 0, 1 \}$ such that $f(x) = b$ for all remaining $x$ such that $\neg f(x) = f(x)$; since $\neg \ell(x) = f(x)$ for all remaining $x$, then we can simply end the decision list.
    
    Therefore,
    $$ |\mathcal C| = \sum_{k = 1}^n k! 2^k 2^k \leq n \cdot n! 4^n \leq n \cdot n^n 4^n = n^{n + 1} 4^n. $$
    It follows that
    $$ M = \frac{1}{\varepsilon} \left(\ln |\mathcal C| + \ln \frac{1}{\delta}\right) \leq \frac{1}{\varepsilon} \left(\ln \left(n^{n + 1} 4^n\right) + \ln \frac{1}{\delta}\right) = O\left(\frac{1}{\varepsilon}\left(n \log n + \log \frac{1}{\delta}\right)\right). $$
    It is easy to see that Algorithm \ref{alg:4b} runs in $O(|S| \cdot 2n \cdot n) = O(Mn^2) = O((n^2/\varepsilon)(n \log n + \log(1/\delta)))$ time, which is polynomial in $n$, $1/\varepsilon$ and $\log(1/\delta)$.

    By Occam's Razor, the probability that any decision list $h$ such that $\PP_{x \in \mathcal D}[f(x) \neq h(x)] \geq \varepsilon$ is consistent with the samples with probability at most $\delta$. This completes the proof.
  \end{proof}
\end{enumerate}

\end{document}
