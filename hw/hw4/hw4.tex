
\documentclass[letterpaper, reqno,11pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{color,latexsym,amsmath,amssymb}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage[linesnumbered,lined,boxed,commentsnumbered,noend,noline]{algorithm2e}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{framed}
\usepackage{titling}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}

\tikzset{invclip/.style={clip,insert path={{[reset cm]
  (-16383.99999pt,-16383.99999pt) rectangle (16383.99999pt,16383.99999pt)}}}}

\allowdisplaybreaks

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\PP}{\mathop{{}\mathbb{P}}}
\newcommand{\EE}{\mathop{{}\mathbb{E}}}
\newcommand{\inci}{\mathds{1}}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\charcone}{char.cone}
\DeclareMathOperator{\STAB}{STAB}
\DeclareMathOperator{\Down}{Down}
\DeclareMathOperator{\lca}{lca}
\DeclareMathOperator{\ex}{ex}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\comp}{c}
\DeclareMathOperator{\bernoulli}{Bernoulli}
\DeclareMathOperator{\Mod}{mod}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\LowestOne}{\textsc{LowestOne}}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\error}{error}
\SetKwFor{RepTimes}{repeat}{times}{end}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\begin{document}
\pagenumbering{arabic}
\title{Homework 4}
\author{Yuchong Pan}
\date{\today}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{exercise}{Exercise}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{solution}{Solution}
%\maketitle
%

\begin{framed}
\noindent{\bf 6.842 Randomness and Computation} \hfill \thedate
\begin{center}
\Large{\thetitle}
\end{center}
%\noindent{\em Lecturer: Ronitt Rubinfield} \hfill {\em Scribe: \theauthor}
\noindent{\em Yuchong Pan} \hfill {\em MIT ID: 911346847}
\end{framed}

\begin{enumerate}
  \item \noindent\emph{Collaborators and sources:} Guanghao Ye, Zixuan Xu.
  
  \begin{proof}
    Let $L$ be a subset of the left vertices such that $|L| \leq n/2$. If $L = \emptyset$, then the result trivially holds. Hence, assume that $|L| \geq 1$. Let $R_0$ be the set of the right vertices. Then
    \begin{align*}
      \PP[|N(L)| < (1 + \varepsilon) |L|] &\leq \PP\left[\exists R \subset R_0, |R| = \lfloor (1 + \varepsilon)|L| \rfloor, N(L) \subset R\right] \\
      &\leq \sum_{\substack{R \subset R_0 \\ |R| = \lfloor (1 + \varepsilon) |L| \rfloor}} \PP[N(L) \subset R] && \text{(union bound)} \\
      &= \sum_{\substack{R \subset R_0 \\ |R| = \lfloor (1 + \varepsilon) |L| \rfloor}} \left(\frac{|R|}{n} \cdot \frac{|R| - 1}{n - 1} \cdots \frac{|R| - |L| + 1}{n - |L| + 1}\right)^3 \\
      &\leq \binom{n}{\lfloor (1 + \varepsilon) |L| \rfloor} \left(\frac{\lfloor (1 + \varepsilon) |L| \rfloor}{n}\right)^{3|L|} && \text{(for $\varepsilon \leq 1$)} \\
      &\leq \left(\frac{en}{\lfloor (1 + \varepsilon) |L| \rfloor}\right)^{\lfloor (1 + \varepsilon) |L| \rfloor} \left(\frac{\lfloor (1 + \varepsilon) |L| \rfloor}{n}\right)^{3|L|} && \text{(Stirling's approximation)} \\
      &\leq \left(\frac{en}{\lfloor (1 + \varepsilon) |L| \rfloor}\right)^{(1 + \varepsilon) |L|} \left(\frac{\lfloor (1 + \varepsilon) |L| \rfloor}{n}\right)^{3|L|} && \text{(for $\varepsilon \leq 2e - 1$)} \\
      &= \left(e^{1 + \varepsilon} \left(\frac{\lfloor (1 + \varepsilon) |L| \rfloor}{n}\right)^{2 - \varepsilon}\right)^{|L|} \\
      &\leq \left(e^{1 + \varepsilon} \left(\frac{(1 + \varepsilon) |L|}{n}\right)^{2 - \varepsilon}\right)^{|L|} \\
      &\leq \left(e^{1 + \varepsilon} \left(\frac{(1 + \varepsilon)}{2}\right)^{2 - \varepsilon}\right)^{|L|}. && \text{(since $|L| \leq n/2$)}
    \end{align*}
    Let $\varepsilon = 1/2$. Then $0 < e^{1 + \varepsilon} ((1 + \varepsilon)/2)^{2 - \varepsilon} < 1/2$. Since $|L| \geq 1$, then
    \begin{align*}
      \PP[|N(L)| \geq (1 + \varepsilon) |L|] &\geq 1 - \PP[|N(L)| < (1 + \varepsilon) |L|] \\
      &\geq 1 - \left(e^{1 + \varepsilon} \left(\frac{(1 + \varepsilon)}{2}\right)^{2 - \varepsilon}\right)^{|L|} \\
      &\geq 1 - e^{1 + \varepsilon} \left(\frac{(1 + \varepsilon)}{2}\right)^{2 - \varepsilon} \\
      &> 1 - \frac{1}{2} = \frac{1}{2}.
    \end{align*}
    This completes the proof.
  \end{proof}

  \clearpage

  \item \begin{enumerate}
    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Note that $\mathds 1_\text{test accepts} = (1 + f(x)f(y)f(z))/2$. By the Fourier transform of $f$ and by linearity of expecation,
      \begin{align*}
        \EE[f(x) f(y) f(z)] &= \EE\left[\left(\sum_{S \subset [n]} \hat{f}(S) \chi_S(x)\right) \left(\sum_{T \subset [n]} \hat{f}(T) \chi_T(y)\right) \left(\sum_{U \subset [n]} \hat{f}(U) \chi_U(z)\right)\right] \\
        &= \sum_{S, T, U \subset [n]} \hat{f}(S) \hat{f}(T) \hat{f}(U) \EE\left[\chi_S(x) \chi_T(y) \chi_U(x \circ y \circ w)\right].
      \end{align*}
      Let $S, T, U \subset [n]$. For all $i \in [n]$, since $x_i, y_i \in \{ \pm 1 \}$, then $x_i^2 = y_i^2 = 1$. Hence,
      \begin{align*}
        \chi_S(x) \chi_T(y) \chi_U(x \circ y \circ w) &= \left(\prod_{i \in S} x_i\right) \left(\prod_{i \in T} y_i\right) \left(\prod_{i \in U} x_i y_i w_i\right) \\
        &= \left(\prod_{i \in S \cap U} x_i^2\right) \left(\prod_{i \in T \cap U} y_i^2\right) \left(\prod_{i \in S \triangle U} x_i\right) \left(\prod_{i \in T \triangle U} y_i\right) \left(\prod_{i \in U} w_i\right) \\
        &= \chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w).
      \end{align*}
      If $S = T = U$, since $w_1, \ldots, w_n$ are all chosen independently and since $\EE[w_i] = (-1) \cdot \delta + 1 \cdot (1 - \delta) = 1 - 2\delta$ for all $i \in [m]$, then
      \begin{align*}
        \EE\left[\chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w)\right] &= \EE\left[\prod_{i \in S} w_i\right] = \prod_{i \in S} \EE\left[w_i\right] = (1 - 2\delta)^{|S|}.
      \end{align*}
      Now, suppose that either $S \neq U$ or $T \neq U$. WLOG assume that $S \neq U$. Then $S \triangle U \neq \emptyset$. Let $j \in S \triangle U$. For $x \in \{ \pm 1 \}^n$, let $x^{\oplus j}$ be the vector obtained by flipping the $j^\text{th}$ bit in $x$. Then we can partition $\{ \pm 1 \}^n$ into (unordered) pairs $(x, x^{\oplus j})$. Therefore,
      \begin{align*}
        \EE\left[\chi_{S \triangle U}(x)\right] &= \frac{1}{2^n} \sum_{x \in \{ \pm 1 \}^n} \chi_{S \triangle U}(x) = \frac{1}{2^n} \sum_{\text{pairs $\left(x, x^{\oplus j}\right)$}} \left(\chi_{S \triangle U}(x) + \chi_{S \triangle U}\left(x^{\oplus j}\right)\right) \\
        &= \frac{1}{2^n} \sum_{\text{pairs $\left(x, x^{\oplus j}\right)$}} \left(x_j \prod_{i \in (S \triangle U) \setminus \{ j \}} x_i + \left(-x_j\right) \prod_{i \in (S \triangle U) \setminus \{ j \}} x_i\right) = 0.
      \end{align*}
      Since $x$, $y$ and $w$ are chosen independently, then for all $S, T, U \subset [n]$ such that either $S \neq U$ or $T \neq U$,
      $$ \EE\left[\chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w)\right] = \EE\left[\chi_{S \triangle U}(x)\right] \EE\left[\chi_{T \triangle U}(y)\right] \EE\left[\chi_U(w)\right] = 0. $$
      Therefore,
      \begin{align*}
        \PP[\text{test accepts}] &= \EE\left[\mathds 1_\text{test accepts}\right] = \EE\left[\frac{1 + f(x)f(y)f(z)}{2}\right] = \frac{1}{2} + \frac{1}{2} \EE[f(x)f(y)f(z)] \\
        &= \frac{1}{2} + \frac{1}{2} \sum_{S, T, U \subset [n]} \hat{f}(S) \hat{f}(T) \hat{f}(U) \EE\left[\chi_{S \triangle U}(x) \chi_{T \triangle U}(y) \chi_U(w)\right] \\
        &= \frac{1}{2} + \frac{1}{2} \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3.
      \end{align*}
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be a dictator function. Then $f = \chi_{\{ j \}}$ for some $j \in [n]$. Therefore, $\hat{f}(\{ j \}) = 1$ and $\hat{f}(S) = 0$ for all $S \subset [n]$ with $S \neq \{ j \}$. By part (a),
      \begin{align*}
        \PP[\text{test accepts}] &= \frac{1}{2} + \frac{1}{2} \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3 \\
        &= \frac{1}{2} + \frac{1}{2} \left((1 - 2\delta)^{|\{ j \}|} \hat{f}(\{ j \})^3 + \sum_{\substack{S \subset [n] \\ S \neq \{ j \}}} (1 - 2\delta)^{|S|} \hat{f}(S)^3\right) \\
        &= \frac{1}{2} + \frac{1}{2} \left((1 - 2\delta)^1 \cdot 1^3 + \sum_{\substack{S \subset [n] \\ S \neq \{ j \}}} (1 - 2\delta)^{|S|} \cdot 0^3\right) \\
        &= \frac{1}{2} + \frac{1}{2} (1 - 2\delta) = 1 - \delta.
      \end{align*}
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be such that $f$ passes with probability at least $1 - \varepsilon$ for some $\varepsilon \in (0, 1/2)$. By part (a),
      $$ 1 - \varepsilon \leq \PP[\text{test accepts}] = \frac{1}{2} + \frac{1}{2} \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3. $$
      Rearranging the above inequality and applying Parseval's identity yield
      \begin{align*}
        1 - 2\varepsilon &\leq \sum_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)^3 \leq \left(\max_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)\right) \sum_{S \subset [n]} \hat{f}(S)^2 \\
        &= \left(\max_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S)\right) \cdot 1 = \max_{S \subset [n]} (1 - 2\delta)^{|S|} \hat{f}(S).
      \end{align*}
      Hence, there exists $S \subset [n]$ such that $(1 - 2\delta)^{|S|} \hat{f}(S) \geq 1 - 2\varepsilon$. Set $\delta = \varepsilon$ in the test. Then $(1 - 2\varepsilon)^{|S|} \hat{f}(S) \geq 1 - 2\varepsilon$. Since $\varepsilon \in (0, 1/2)$, then $1 - 2\varepsilon \in (0, 1)$, so $(1 - 2\varepsilon)^{|S|} \in (0, 1]$. Therefore,
      $$ \hat{f}(S) \geq \frac{1 - 2\varepsilon}{(1 - 2\varepsilon)^{|S|}} \geq \frac{1 - 2\varepsilon}{1} = 1 - 2\varepsilon. $$
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} none.
    
    By part (c), if $f$ passes with probability at least $1 - \varepsilon$ for some $\varepsilon \in (0, 1/2)$, then there exists $S \subset [n]$ such that $(1 - 2\varepsilon)^{|S|} \hat{f}(S) \geq 1 - 2\varepsilon$ by setting $\delta = \varepsilon$ in the test. Since $\dist(f, \chi_S) \in [0, 1]$, then $\hat{f}(S) = 1 - 2\dist(f, \chi_S) \in [-1, 1]$. Since $\varepsilon \in (0, 1/2)$, then $1 - 2\varepsilon \in (0, 1)$. If $|S| \geq 2$, then $0 < (1 - 2\varepsilon)^{|S|} < 1 - 2\varepsilon$, so $(1 - 2\varepsilon)^{|S|} \hat{f}(S) < 1 - 2\varepsilon$, a contradiction. Therefore, one of the following two cases holds:
    \begin{enumerate}[label=(\roman*), itemsep=0pt]
      \item $|S| = 1$ and $\hat{f}(S) = 1$ (so $\dist(f, \chi_S) = 0$, and $f = \chi_S$ is a dictator function);
      \item $|S| = 0$ and $\hat{f}(S) \geq 1 - 2\varepsilon$ (so $\dist(f, \chi_\emptyset) \leq \varepsilon$).
    \end{enumerate}
    Hence, if $f$ is $\varepsilon$-close to $\chi_\emptyset \equiv 1$ (a non-dictator function), then $f$ also passes with probability at least $1 - \varepsilon$.

    Note that for any dictator function, say $\chi_{\{ j \}}$ for some $j \in [n]$,
    $$ \PP_{x \in \{ \pm 1 \}^n}\left[\chi_{\{ j \}}(x) = 0\right] = \PP_{x \in \{ \pm 1 \}^n}\left[x_j = 0\right] = \frac{\left|\left\{ x \in \{ \pm 1 \}^n : x_j = 0 \right\}\right|}{2^n} = \frac{2^{n - 1}}{2^n} = \frac{1}{2}. $$
    In other words, any dictator function equals $0$ for half of the inputs, and $1$ for the other half. We give a simple fix to the test by applying the following new test before the original test. For any sufficiently small $\eta > 0$, we independently and uniformly sample $\Theta(\log (1/\eta))$ random inputs from $\{ \pm 1 \}^n$, and reject if and only if more than $3/4$ of the values are $1$. If $f$ is $\varepsilon$-close to $\chi_\emptyset \equiv 1$ for some $\varepsilon \in (0, 1/8)$, then by the Chernoff bound,
    $$ \PP[\text{new test rejects $f$}] = 1 - \PP[\text{$\leq 3/4$ of the values are $1$}] \geq 1 - e^{-\Theta(\log (1/\eta))} = 1 - \Theta(\eta). $$
    On the other hand, if $f$ is a dictator function, then by the Chernoff bound,
    $$ \PP[\text{new test accepts $f$}] = 1 - \PP[\text{$>3/4$ of the values are $1$}] \geq 1 - e^{-\Theta(\log(1/\eta))} = 1 - \Theta(\eta). $$
    Hence, if $f$ passes the combination of the new test and the original test with probability at least $1 - \varepsilon$ and with $\delta = \varepsilon$ in the original test for some sufficiently small $\varepsilon > 0$, then $f$ is a dictator function with probability at least $1 - \Theta(\eta)$; on the other hand, if $f$ is a dictator function, then the union bound implies that $f$ passes the combined test with probability at least $1 - \Theta(\eta) - \delta$. This shows that the combined test is a dictator test.
  \end{enumerate}
  
  \clearpage

  \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
  
  \begin{proof}
    Let $\mathcal A$ be a PAC learning algorithm for a class $C$ that runs in $\poly(\log n, 1/\varepsilon, 1/\delta)$ time. We denote by $\error_{\mathcal D}(h) = \PP_{x \sim \mathcal D}[h(x) \neq f(x)]$ the error of a hypothesis $h$ with respect to $f$ with inputs drawn from distribution $\mathcal D$. We denote by $\error_S(h) = |\{ x \in S : h(x) \neq f(x) \}|/|S|$ the error of $h$ in the sample set $S$. We give a PAC learning algorithm in Algorithm \ref{alg:confidence} with running time $\poly(\log n, 1/\varepsilon, \log(1/\delta))$. Let $\mathcal D$ be the distribution of inputs.

    \begin{algorithm}
      $\ell \leftarrow \lceil\log_2 (3/\delta) \rceil$ \\
      \ForEach{$i \leftarrow 1, \ldots, \ell$}{
        run $\mathcal A$ with accuracy $\varepsilon/2$ and confidence $1/2$, obtaining a hypothesis $h_i$
      }
      $m \leftarrow \lceil (12/\varepsilon^2) \log (6\ell/\delta) \rceil$ \\
      \ForEach{$j \leftarrow 1, \ldots, m$}{
        draw $x_j \sim \mathcal D$
      }
      $S \leftarrow \{ x_1, \ldots, x_m \}$ \\
      $i^* \leftarrow \argmin_{i \in [\ell]}(\error_S(h_i))$ \\
      \Return{$h_{i^*}$}
      \caption{A PAC learning algorithm with running time $\poly(\log n, 1/\varepsilon, \log(1/\delta))$, given accuracy $\varepsilon > 0$ and confidence $\delta > 0$.}
      \label{alg:confidence}
    \end{algorithm}

    Since each call to $\mathcal A$ runs in $\poly(\log n, 1/\varepsilon)$ time, then the running time of Algorithm \ref{alg:confidence} is
    $$ O\left(\log \frac{1}{\delta}\right) \poly\left(\log n, \frac{1}{\varepsilon}\right) + O\left(\frac{1}{\varepsilon^2} \log \frac{\log \frac{1}{\delta}}{\delta}\right) = \poly\left(\log n, \frac{1}{\varepsilon}, \log \frac{1}{\delta}\right). $$

    First, since $\PP[\error_{\mathcal D}(h_i) \leq \varepsilon/2] \geq 1 - 1/2 = 1/2$ for each $i \in [\ell]$, then
    $$ \PP\left[\exists i \in [\ell], \error_{\mathcal D}(h_i) \leq \varepsilon/2\right] \geq 1 - \left(1 - \frac{1}{2}\right)^\ell = 1 - \left(\frac{1}{2}\right)^{\lceil\log_2 \left(\frac{3}{\delta}\right) \rceil} \geq 1 - \left(\frac{1}{2}\right)^{\log_2 \left(\frac{3}{\delta}\right)} = 1 - \frac{\delta}{3}. $$
    Second,
    \begin{align*}
      &\quad\, \PP\left[\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| < \frac{\varepsilon}{4} \; \forall i \in [\ell]\right] \\
      &= 1 - \PP\left[\exists i \in [\ell], \left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| \geq \frac{\varepsilon}{4}\right] \\
      &\geq 1 - \sum_{i = 1}^\ell \PP\left[\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| \geq \frac{\varepsilon}{4}\right] && \text{(union bound)} \\
      &= 1 - \sum_{i = 1}^\ell \PP\left[\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| \geq \frac{\varepsilon}{4\error_{\mathcal D}\left(h_i\right)} \cdot \error_{\mathcal D}\left(h_i\right)\right] \\
      &\geq 1 - \sum_{i = 1}^\ell 2\exp\left(-\frac{1}{3} m\error_{\mathcal D}\left(h_i\right) \cdot \left(\frac{\varepsilon}{4\error_{\mathcal D}\left(h_i\right)}\right)^2\right) && \text{(Chernoff bound)} \\
      &= 1 - \sum_{i = 1}^\ell 2\exp\left(-\frac{m\varepsilon^2}{12 \error_{\mathcal D}\left(h_i\right)}\right) \\
      &\geq 1 - \ell \cdot 2\exp\left(-\frac{\left\lceil\frac{12}{\varepsilon^2} \log \frac{6\ell}{\delta}\right\rceil \cdot \varepsilon^2}{12 \cdot 1}\right) \\
      &\geq 1 - 2\ell\exp\left(-\frac{\frac{12}{\varepsilon^2} \log \frac{6\ell}{\delta} \cdot \varepsilon^2}{12}\right) \\
      &= 1 - 2\ell\exp\left(-\log \frac{6\ell}{\delta}\right) \\
      &= 1 - 2\ell \cdot \frac{\delta}{6\ell} \\
      &= 1 - \frac{\delta}{3}.
    \end{align*}
    Since $i^*$ minimizes $\error_S(h_i)$ over $i \in [\ell]$, then by the union bound,
    \begin{align*}
      \PP\left[\error_S\left(h_{i^*}\right) < \frac{3\varepsilon}{4}\right] &\geq \PP\left[\exists i \in [\ell], \error_S\left(h_i\right) < \frac{3\varepsilon}{4}\right] \\
      &\geq \PP\left[\left(\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| < \frac{\varepsilon}{4} \;\forall i \in [\ell]\right) \wedge \left(\exists i \in [\ell], \error_{\mathcal D}\left(h_i\right) \leq \frac{\varepsilon}{2}\right)\right] \\
      &\geq 1 - \left(\frac{\delta}{3} + \frac{\delta}{3}\right) \\
      &= 1 - \frac{2\delta}{3}.
    \end{align*}
    By the union bound again,
    \begin{align*}
      \PP\left[\error_{\mathcal D}\left(h_{i^*}\right) < \varepsilon\right] &\geq \PP\left[\left(\left|\error_{\mathcal D}\left(h_i\right) - \error_S\left(h_i\right)\right| < \frac{\varepsilon}{4} \;\forall i \in [\ell]\right) \wedge \left(\error_S\left(h_{i^*}\right) < \frac{3\varepsilon}{4}\right)\right] \\
      &\geq 1 - \left(\frac{\delta}{3} + \frac{2\delta}{3}\right) \\
      &= 1 - \delta.
    \end{align*}
    This completes the proof.
  \end{proof}
\end{enumerate}

\end{document}
