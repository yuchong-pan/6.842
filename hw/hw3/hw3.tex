
\documentclass[letterpaper, reqno,11pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{color,latexsym,amsmath,amssymb}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage[linesnumbered,lined,boxed,commentsnumbered,noend,noline]{algorithm2e}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{framed}
\usepackage{titling}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}

\tikzset{invclip/.style={clip,insert path={{[reset cm]
  (-16383.99999pt,-16383.99999pt) rectangle (16383.99999pt,16383.99999pt)}}}}

\allowdisplaybreaks

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\PP}{\mathop{{}\mathbb{P}}}
\newcommand{\EE}{\mathop{{}\mathbb{E}}}
\newcommand{\inci}{\mathds{1}}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\charcone}{char.cone}
\DeclareMathOperator{\STAB}{STAB}
\DeclareMathOperator{\Down}{Down}
\DeclareMathOperator{\lca}{lca}
\DeclareMathOperator{\ex}{ex}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\comp}{c}
\DeclareMathOperator{\bernoulli}{Bernoulli}
\DeclareMathOperator{\Mod}{mod}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\LowestOne}{\textsc{LowestOne}}
\SetKwFor{RepTimes}{repeat}{times}{end}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\begin{document}
\pagenumbering{arabic}
\title{Homework 2}
\author{Yuchong Pan}
\date{\today}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{exercise}{Exercise}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{solution}{Solution}
%\maketitle
%

\begin{framed}
\noindent{\bf 6.842 Randomness and Computation} \hfill \thedate
\begin{center}
\Large{\thetitle}
\end{center}
%\noindent{\em Lecturer: Ronitt Rubinfield} \hfill {\em Scribe: \theauthor}
\noindent{\em Yuchong Pan} \hfill {\em MIT ID: 911346847}
\end{framed}

\begin{enumerate}
  \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
  
  \begin{proof}
    Let $\mathbf x^* = \langle x_1^*, \ldots, x_n^* \rangle$ be a satisfying assignment, and let $\mathbf x = \langle x_1, \ldots, x_n \rangle$ be the assignment in the algorithm. We denote by $d(\mathbf x^*, \mathbf x)$ the number of locations at which $\mathbf x^*$ and $\mathbf x$ differ for any assignment $\mathbf x$. Consider an iteration of the algorithm that picks an unsatisfied clause $C_k$ involving variables $X_{k_1}$ and $X_{k_2}$. We say that a variable $X_{k_i}$ is \emph{tight} for clause $C_k$ if its corresponding literal in $C_k$ evaluates to \emph{true}; otherwise we say that it is \emph{slack} for $C_k$. Then $X_{k_1}$ and $X_{k_2}$ cannot be both slack with respect to $\mathbf x^*$, and $X_{k_1}$ and $X_{k_2}$ must be both slack before the modification in the iteration. Table \ref{tab:1} indicates the change of $d(\mathbf x^*, \mathbf x)$ for each combination of the tightnesses/slacknesses of $X_{k_1}$ and $X_{k_2}$ with respect to $\mathbf x^*$ and $\mathbf x$, respectively.

    \begin{table}[h]
      \centering
      \begin{tabular}{c||c|c|c}
        $(X_{k_1}, X_{k_2})$ & (slack, tight) & (tight, slack) & (tight, tight) \\
        \hline
        \hline
        (slack, tight) & $1 \to 0$ & $1 \to 2$ & $2 \to 1$ \\
        \hline
        (tight, slack) & $1 \to 2$ & $1 \to 0$ & $2 \to 1$
      \end{tabular}
      \caption{Indicating the change of $d(\mathbf x^*, \mathbf x)$ for each combination of the tightnesses/slacknesses of $X_{k_1}$ and $X_{k_2}$ with respect to $\mathbf x^*$ and $\mathbf x$, respectively, where rows correspond to combinations with respect to $\mathbf x$ after the modification in the iteration, columns correspond to combinations with respect to $\mathbf x^*$, and each entry indicates the change of $d(\mathbf x^*, \mathbf x)$ for $X_{k_1}$ and $X_{k_2}$.}
      \label{tab:1}
    \end{table}

    Since the algorithm complements one of the two literals uniformly at random, then Table \ref{tab:1} implies that $d(\mathbf x^*, \mathbf x)$ decreases by $1$ with probability $p_- \geq 1/2$, and increases by $1$ with probability at most $p_+ \leq 1/2$, such that $p_- + p_+ = 1$.

    Let $G = (V, E)$ be a path graph with $V = \{ 0, \ldots, n \}$ and $E = \{ (i - 1, i) : i \in [n] \}$, where vertex $i$ corresponds to the value of $d(\mathbf x^*, \mathbf x)$ in the algorithm. Let $d_0$ be the value of $d(\mathbf x^*, \mathbf x)$ at the beginning of the algorithm. Consider the following stochastic process: Start at vertex $d_0$; in each iteration, move to the left or to the right according to the change of $d(\mathbf x^*, \mathbf x)$ in the iteration. For all $i, j \in V$, let $h(i, j)$ be the expected time needed to reach $j$ (for the first time) from $i$. Then $h(n, n - 1) = 1$. For each $i \in [n - 1]$,
    \begin{align}
      h(i, i - 1) &= \PP[i \to i - 1] \cdot 1 + \PP[i \to i + 1] \cdot (1 + h(i + 1, i - 1)) \nonumber \\
      &\leq \frac{1}{2} \cdot 1 + \frac{1}{2}\left(1 + h(i + 1, i - 1)\right) \label{eq:1} \\
      &\leq 1 + \frac{1}{2}(h(i + 1, i) + h(i, i - 1)). \nonumber
    \end{align}
    Note that \eqref{eq:1} follows from the facts that $h(i + 1, i - 1) \geq 0$, that $\PP[i \to i - 1] \geq 1/2$ and that $\PP[i \to i + 1] \leq 1/2$. Therefore, $h(i, i - 1) \leq h(i + 1, i) + 2$ for each $i \in [n - 1]$. Solving this recurrence relation gives $h(i, i - 1) \leq 2(n - i) + 1$ for each $i \in [n]$. It follows that
    $$ h\left(d_0, 0\right) \leq \sum_{i = 1}^{d_0} h(i, i - 1) \leq \sum_{i = 1}^n h(i, i - 1) \leq \sum_{i = 1}^n (2(n - i) + 1) = \frac{((2n - 1) + 1) \cdot n}{2} = n^2. $$
    Let $Z$ be the minimum value of $s$ needed for a specific execution of the algorithm to output a satisfying assignment. Then $\EE[Z] = h(d_0, 0) \leq n^2$. By Markov's inequality,
    $$ \PP\left[Z \geq 4n^2\right] \leq \frac{\EE[Z]}{4n^2} \leq \frac{n^2}{4n^2} = \frac{1}{4}. $$
    Therefore, if $s = 4n^2$, then the algorithm will output a satisfying assignment with probability at least $3/4$. This completes the proof.
  \end{proof}
  
  \clearpage

  \item \begin{enumerate}
    \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
    
    \begin{proof}
      Let $\{ x, y \} \subset A$ be such that $x \neq y$. Then for any pairwise independent hash function $h \in B$,
      $$ (h(x), h(y)) \in_U T^2. $$
      Therefore,
      \begin{equation} \label{eq:2a-prob}
        \PP_{h \in_U B}[h(x) = h(y)] = \sum_{z \in T} \PP_{h \in_U B}[(h(x), h(y)) = (z, z)] = |T| \cdot \frac{1}{\left|T^2\right|} = t \cdot \frac{1}{t^2} = \frac{1}{t}.
      \end{equation}
      It follows that
      \begin{align*}
        \EE_{h \in_U B}[\text{\# colliding pairs for $h$}] &= \EE_{h \in_U B}\left[\sum_{\substack{\{ x, y \} \subset A \\ x \neq y}} \mathds 1_{\text{$\{ x, y \}$ is a colliding pair for $h$}}\right] \\
        &= \sum_{\substack{\{ x, y \} \subset A \\ x \neq y}} \EE_{h \in_U B}\left[\mathds 1_{\text{$\{ x, y \}$ is a colliding pair for $h$}}\right] \\
        &= \sum_{\substack{\{ x, y \} \subset A \\ x \neq y}} \PP_{h \in_U B}\left[\text{$\{ x, y \}$ is a colliding pair for $h$}\right] \\
        &= \sum_{\substack{\{ x, y \} \subset A \\ x \neq y}} \PP_{h \in_U B}[h(x) = h(y)] \\
        &= |\{ \{ x, y \} \subset A : x \neq y \}| \cdot \frac{1}{t} \\
        &= \binom{|A|}{2} \cdot \frac{1}{t} \\
        &= \binom{n}{2} \cdot \frac{1}{t}.
      \end{align*}
      This completes the proof.
    \end{proof}
    
    \clearpage

    \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
    
    \begin{proof}
      Let $p = (p_i)_{i \in A}$ be a distribution over $A$ such that $c(p) \leq (1 + \varepsilon^2)/|A|$ for some $\varepsilon > 0$. Then $\sum_{i \in A} p_i = 1$ and $\sum_{i \in A} p_i^2 \leq (1 + \varepsilon^2)/|A|$. Therefore,
      \begin{align*}
        \left\| p - U_A \right\|_1 &\leq \sqrt{|A|} \left\| p - U_A \right\|_2 && \text{(Cauchy-Schwarz inequality)} \\
        &= \sqrt{|A|} \sqrt{\sum_{i \in A} \left(p_i - \frac{1}{|A|}\right)^2} \\
        &= \sqrt{|A|} \sqrt{\sum_{i \in A} \left(p_i^2 - \frac{2p_i}{|A|} + \frac{1}{|A|^2}\right)} \\
        &= \sqrt{|A|} \sqrt{\sum_{i \in A} p_i^2 - \frac{2}{|A|}\sum_{i \in A} p_i + \sum_{i \in A} \frac{1}{|A|^2}} \\
        &\leq \sqrt{|A|} \sqrt{\frac{1 + \varepsilon^2}{|A|} - \frac{2}{|A|} \cdot 1 + |A| \cdot \frac{1}{|A|^2}} \\
        &= \sqrt{|A|} \sqrt{\frac{1 + \varepsilon^2}{|A|} - \frac{2}{|A|} + \frac{1}{|A|}} \\
        &= \sqrt{|A| \cdot \frac{1 + \varepsilon^2 - 2 + 1}{|A|}} \\
        &= \sqrt{\varepsilon^2} \\
        &= \varepsilon.
      \end{align*}
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
    
    \begin{proof}
      Let $q$ be a distribution over $B \times T$ be defined as in the problem. Let $x, y \in A$. If $x = y$, then $h(x) = h(y)$ for any $h \in B$. If $x \neq y$, then \eqref{eq:2a-prob} implies that for any $h \in B$,
      $$ \PP_{x, y \in_U W}[h(x) = h(y) \mid x \neq y] = \frac{1}{t} = \frac{1}{|T|}. $$
      For any set $\Omega$,
      \begin{align*}
        \PP_{\omega_1, \omega_2 \in_U \Omega}\left[\omega_1 = \omega_2\right] &= \sum_{\omega \in \Omega} \PP_{\omega_1, \omega_2 \in_U \Omega}\left[\omega_1 = \omega_2 = \omega\right] \\
        &= \sum_{\omega \in \Omega} \PP_{\omega_1 \in_U \Omega}\left[\omega_1 = \omega\right] \PP_{\omega_2 \in_U \Omega}\left[\omega_2 = \omega\right] && \text{(independence)} \\
        &= |\Omega| \cdot \frac{1}{|\Omega|} \cdot \frac{1}{|\Omega|} \\
        &= \frac{1}{|\Omega|}.
      \end{align*}
      This implies that $\PP_{h_1, h_2 \in_U B}[h_1 = h_2] = 1/|B|$ and that $\PP_{x_1, x_2 \in_U W}[x_1 = x_2] = 1/|W|$. Fix $h \in B$. Then
      \begin{align*}
        \PP_{x_1, x_2 \in_U W}\left[h\left(x_1\right) = h\left(x_2\right)\right] &= \PP_{x_1, x_2 \in_U W}\left[x_1 = x_2\right] \PP_{x_1, x_2 \in_U W}\left[h\left(x_1\right) = h\left(x_2\right) \;\middle|\; x_1 = x_2\right] + \\
        &\quad\; \PP_{x_1, x_2 \in_U W}\left[x_1 \neq x_2\right] \PP_{x_1, x_2 \in_U W}\left[h\left(x_1\right) = h\left(x_2\right) \;\middle|\; x_1 \neq x_2\right] \\
        &\leq \frac{1}{|W|} \cdot 1 + 1 \cdot \frac{1}{|T|} \\
        &= \frac{1}{|W|} + \frac{1}{|T|}.
      \end{align*}
      Therefore,
      \begin{align*}
        c(q) &= \PP_{\left\langle h_1, y_1 \right\rangle, \left\langle h_2, y_2 \right\rangle \in_q B \times T}\left[\left\langle h_1, y_1 \right\rangle = \left\langle h_2, y_2 \right\rangle\right] \\
        &= \PP_{\substack{h_1, h_2 \in_U B \\ x_1, x_2 \in_U W}}\left[h_1 = h_2, h_1\left(x_1\right) = h_2\left(x_2\right)\right] \\
        &= \PP_{h_1, h_2 \in_U B}\left[h_1 = h_2\right] \PP_{\substack{h_1, h_2 \in_U B \\ x_1, x_2 \in_U W}} \left[h_1\left(x_1\right) = h_2\left(x_2\right) \;\middle|\; h_1 = h_2\right] && \text{(independence)} \\
        &= \frac{1}{|B|} \PP_{\substack{h \in B \\ x_1, x_2 \in_U W}} \left[h\left(x_1\right) = h\left(x_2\right) \;\middle|\; h\right] \\
        &\leq \frac{1}{|B|} \left(\frac{1}{|W|} + \frac{1}{|T|}\right) \\
        &= \frac{1}{|B|} \cdot \frac{|T|/|W| + 1}{|T|} \\
        &= \frac{1 + |T|/|W|}{|B| \cdot |T|} \\
        &= \frac{1 + |T|/|W|}{|B \times T|}.
      \end{align*}
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
    
    \begin{proof}
      Note that it follows from the same argument of part (b) that for any distribution $\mu$ over any finite set $\Omega$, if $c(\mu) \leq (1 + \varepsilon^2)/|\Omega|$ for some $\varepsilon > 0$, then $\|\mu - U_\Omega\|_1 \leq \varepsilon$. Let $\Omega = B \times T$. Let $\varepsilon = \sqrt{|T|/|W|} > 0$. Then $|T|/|W| = \varepsilon^2$. By part (c),
      $$ c(q) \leq \frac{1 + |T|/|W|}{|B \times T|} = \frac{1 + \varepsilon^2}{|\Omega|}. $$
      Since $q$ is a distribution over $B \times T = \Omega$, then
      $$ \left\| q - U_{B \times T}\right\|_1 = \left\| q - U_\Omega \right\|_1 \leq \varepsilon = \sqrt{|T|/|W|}. $$
      This completes the proof.
    \end{proof}
  \end{enumerate}
\end{enumerate}

\end{document}
