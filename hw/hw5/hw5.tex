
\documentclass[letterpaper, reqno,11pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{color,latexsym,amsmath,amssymb}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage[linesnumbered,lined,boxed,commentsnumbered,noend,noline]{algorithm2e}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{framed}
\usepackage{titling}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}

\tikzset{invclip/.style={clip,insert path={{[reset cm]
  (-16383.99999pt,-16383.99999pt) rectangle (16383.99999pt,16383.99999pt)}}}}

\allowdisplaybreaks
\DontPrintSemicolon
\SetKw{Break}{break}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\PP}{\mathop{{}\mathbb{P}}}
\newcommand{\EE}{\mathop{{}\mathbb{E}}}
\newcommand{\inci}{\mathds{1}}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\charcone}{char.cone}
\DeclareMathOperator{\STAB}{STAB}
\DeclareMathOperator{\Down}{Down}
\DeclareMathOperator{\lca}{lca}
\DeclareMathOperator{\ex}{ex}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\comp}{c}
\DeclareMathOperator{\bernoulli}{Bernoulli}
\DeclareMathOperator{\Mod}{mod}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\LowestOne}{\textsc{LowestOne}}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\error}{error}
\DeclareMathOperator{\NS}{\mathit{NS}}
\DeclareMathOperator{\Inf}{\mathit{Inf}}
\DeclareMathOperator{\Unif}{\mathsf{Unif}}
\SetKwFor{RepTimes}{repeat}{times}{end}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\begin{document}
\pagenumbering{arabic}
\title{Homework 5}
\author{Yuchong Pan}
\date{\today}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{exercise}{Exercise}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{solution}{Solution}
%\maketitle
%

\begin{framed}
\noindent{\bf 6.842 Randomness and Computation} \hfill \thedate
\begin{center}
\Large{\thetitle}
\end{center}
%\noindent{\em Lecturer: Ronitt Rubinfield} \hfill {\em Scribe: \theauthor}
\noindent{\em Yuchong Pan} \hfill {\em MIT ID: 911346847}
\end{framed}

\begin{enumerate}
  \item \noindent\emph{Collaborators and sources:} none.
  
  \begin{proof}
    Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$. Let $\varepsilon \in (0, 1/2)$. Then
    \begin{align*}
      \NS_\varepsilon(f) &= \PP_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}} \left[f(x) \neq f\left(N_\varepsilon(x)\right)\right] = \PP_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}} \left[f(x) f\left(N_\varepsilon(x)\right) = -1\right] \\
      &= \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\frac{1}{2} - \frac{1}{2}f(x) f\left(N_\varepsilon(x)\right)\right] = \frac{1}{2} - \frac{1}{2} \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[f(x) f\left(N_\varepsilon(x)\right)\right] \\
      &= \frac{1}{2} - \frac{1}{2} \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\left(\sum_{S \subset [n]} \hat{f}(S) \chi_S(x)\right) \left(\sum_{T \subset [n]} \hat{f}(T) \chi_T\left(N_\varepsilon(x)\right)\right)\right] \\
      &= \frac{1}{2} - \frac{1}{2} \sum_{S, T \subset [n]} \hat{f}(S) \hat{f}(T) \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\chi_S(x) \chi_T\left(N_\varepsilon(x)\right)\right].
    \end{align*}
    For all $x \in \{ \pm 1 \}^n$ and $i \in [n]$, we denote by $x_i$ and $N_\varepsilon(x)_i$ the $i^\text{th}$ coordinates of $x$ and $N_\varepsilon(x)$, respectively. For all $S \subset [n]$,
    \begin{align}
      \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\chi_S(x) \chi_S\left(N_\varepsilon(x)\right)\right] &= \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}} \left[\left(\prod_{i \in S} x_i\right)\left(\prod_{i \in S} N_\varepsilon(x)_i\right)\right] \nonumber = \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\prod_{i \in S} x_i N_\varepsilon(x)_i\right] \nonumber \\
      &= \prod_{i \in S} \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}} \left[x_i N_\varepsilon(x)_i\right] = (\varepsilon \cdot (-1) + (1 - \varepsilon) \cdot 1)^{|S|} \label{eq:1-indep-symmetry} \\
      &= (1 - 2\varepsilon)^{|S|}. \nonumber
    \end{align}
    Note that \eqref{eq:1-indep-symmetry} is due to the independence of each bit in $N_\varepsilon(x)$ and the fact that each bit of $x$ uniformly chosen from $\{ \pm 1 \}^n$ is uniform in $\{ \pm 1 \}$. For all $S, T \subset [n]$ with $S \neq T$,
    \begin{align}
      &\quad\, \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\chi_S(x) \chi_T\left(N_\varepsilon(x)\right)\right] \nonumber \\
      &= \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}} \left[\left(\prod_{i \in S} x_i\right)\left(\prod_{i \in T} N_\varepsilon(x)_i\right)\right] \nonumber \\
      &= \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}} \left[\left(\prod_{i \in S \cap T} x_i N_\varepsilon(x)_i\right) \left(\prod_{i \in S \setminus T} x_i\right)\left(\prod_{i \in T \setminus S} N_\varepsilon(x)_i\right)\right] \nonumber \\
      &= \left(\prod_{i \in S \cap T} \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}} \left[x_i N_\varepsilon(x)_i\right]\right) \left(\prod_{i \in S \setminus T} \EE_{x \in \{ \pm 1 \}^n}\left[x_i\right]\right) \left(\prod_{i \in T \setminus S} \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[N_\varepsilon(x)_i\right]\right). \label{eq:1-indep-diff}
    \end{align}
    Note that \eqref{eq:1-indep-diff} is again due to the independence of each bit in $N_\varepsilon(x)$. For $S, T \subset [n]$ with $S \neq T$, either $S \setminus T \neq \emptyset$ or $T \setminus S \neq \emptyset$. Note that each bit of $x$ uniformly chosen from $\{ \pm 1 \}^n$ is uniform in $\{ \pm 1 \}$. Therefore, if $S \setminus T \neq \emptyset$,
    \begin{align*}
      \prod_{i \in S \setminus T} \EE_{x \in \{ \pm 1 \}^n}\left[x_i\right] = \left(\EE_{b \in \{ \pm 1 \}}[b]\right)^{|S \setminus T|} = 0^{|S \setminus T|} = 0.
    \end{align*}
    Moreover, if $T \setminus S \neq \emptyset$,
    $$ \prod_{i \in T \setminus S} \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[N_\varepsilon(x)_i\right] = \left(\frac{1}{2}(\varepsilon (-1) + (1 - \varepsilon) \cdot 1) + \frac{1}{2} (\varepsilon \cdot 1 + (1 - \varepsilon) (-1))\right)^{|T \setminus S|} = 0^{|T \setminus S|} = 0. $$
    Therefore, for all $S, T \subset [n]$ with $S \neq T$,
    $$ \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\chi_S(x) \chi_T\left(N_\varepsilon(x)\right)\right] = 0. $$
    It follows that
    \begin{align*}
      \NS_\varepsilon(f) = \frac{1}{2} - \frac{1}{2} \sum_{S, T \subset [n]} \hat{f}(S) \hat{f}(T) \EE_{\substack{x \in \{ \pm 1 \}^n \\ N_\varepsilon}}\left[\chi_S(x) \chi_T\left(N_\varepsilon(x)\right)\right] = \frac{1}{2} - \frac{1}{2} \sum_{S \subset [n]} \hat{f}(S)^2 (1 - 2 \varepsilon)^{|S|}.
    \end{align*}
    This completes the proof.
  \end{proof}

  \clearpage

  \item \begin{enumerate}
    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be monotone. Let $i \in [n]$. WLOG, assume $i = 1$. Then
      \begin{align*}
        \hat{f}(\{ 1 \}) &= \left\langle f, \chi_{\{ 1 \}} \right\rangle \\
        &= \frac{1}{2^n} \sum_{x \in \{ \pm 1 \}^n} f(x) \chi_{\{ 1 \}}(x) \\
        &= \frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} f(x) x_1 \\
        &= \frac{1}{2^n}\left(\sum_{\substack{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n \\ x_1 = 1}} f(x) \cdot 1 + \sum_{\substack{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n \\ x_1 = -1}} f(x) \cdot (-1)\right) \\
        &= \frac{1}{2^n} \left(\sum_{x' \in \{ \pm 1 \}^{n - 1}} f\left(1, x'\right) - \sum_{x' \in \{ \pm 1 \}^{n - 1}} f\left(-1, x'\right)\right) \\
        &= \frac{1}{2^n} \sum_{x' \in \{ \pm 1 \}^{n - 1}} \left(f\left(1, x'\right) - f\left(-1, x'\right)\right) \\
        &= \frac{1}{2^n} \sum_{\substack{x' \in \{ \pm 1 \}^{n - 1} \\ f\left(1, x'\right) \neq f\left(-1, x'\right)}} \left(f\left(1, x'\right) - f\left(-1, x'\right)\right).
      \end{align*}
      Since $f$ is monotone, then $f(1, x') \geq f(-1, x')$ for all $x' \in \{ \pm 1 \}^{n - 1}$. Hence, for all $x' \in \{ \pm 1 \}^{n - 1}$, if $f(1, x') \neq f(-1, x')$, then $f(1, x') = 1$ and $f(-1, x') = -1$, so $f(1, x') - f(-1, x') = 1 - (-1) = 2$. Therefore,
      \begin{align*}
        \hat{f}(\{ 1 \}) &= \frac{1}{2^n} \sum_{\substack{x' \in \{ \pm 1 \}^{n - 1} \\ f\left(1, x'\right) \neq f\left(-1, x'\right)}} 2 \\
        &= \frac{1}{2^n} \cdot 2 \left|\left\{ x' \in \{ \pm 1 \}^{n - 1} : f\left(1, x'\right) \neq f\left(-1, x'\right) \right\}\right| \\
        &= \frac{1}{2^{n - 1}} \left|\left\{ x' \in \{ \pm 1 \}^{n - 1} : f\left(1, x'\right) \neq f\left(-1, x'\right) \right\}\right|.
      \end{align*}
      On the other hand,
      \begin{align*}
        \Inf_1(f) &= \PP_{x \in \{ \pm 1 \}^n}\left[f(x) \neq f\left(x^{\oplus 1}\right)\right] \\
        &= \frac{1}{2^n} \sum_{x \in \{ \pm 1 \}^n} \mathds 1\left[f(x) \neq f\left(x^{\oplus 1}\right)\right] \\
        &= \frac{1}{2^n} \cdot 2 \sum_{x' \in \{ \pm 1 \}^{n - 1}} \mathds 1\left[f\left(1, x'\right) \neq f\left(-1, x'\right)\right] \\
        &= \frac{1}{2^{n - 1}} \left|\left\{ x' \in \{ \pm 1 \}^{n - 1} : f\left(1, x'\right) \neq f\left(-1, x'\right) \right\}\right| \\
        &= \hat{f}(\{ 1 \}).
      \end{align*}
      This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $n \in \NN$ be odd. Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be the majority function, i.e., $f(x) = \sign(\sum_{i = 1}^n x_i)$ for all $x = (x_1, \ldots, x_n) \in \{ \pm 1 \}^n$. First, we show that $f$ is monotone. Let $x = (x_1, \ldots, x_n), y = (y_1, \ldots, y_n) \in \{ \pm 1 \}^n$ be such that $x_i \leq y_i$ for all $i \in [n]$. Then $\sum_{i = 1}^n x_i \leq \sum_{i = 1}^n y_i$, so $f(x) = \sign(\sum_{i = 1}^n x_i) \leq \sign(\sum_{i = 1}^n y_i) = f(y)$. This proves that $f$ is monotone.

      Second, let $g : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be monotone. Then
      \begin{align*}
        \Inf(g) &= \sum_{i = 1}^n \Inf_i(g) \\
        &= \sum_{i = 1}^n \hat{g}(\{ i \}) && \text{(part (a))} \\
        &= \sum_{i = 1}^n \left\langle g, \chi_{\{ i \}} \right\rangle \\
        &= \sum_{i = 1}^n \frac{1}{2^n} \sum_{x \in \{ \pm 1 \}^n} g(x) \chi_{\{ i \}}(x) \\
        &= \frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} g(x) \sum_{i = 1}^n x_i \\
        &\leq \left|\frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} g(x) \sum_{i = 1}^n x_i\right| \\
        &\leq \frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} |g(x)| \left|\sum_{i = 1}^n x_i\right| && \text{(triangle inequality)} \\
        &= \frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} \left|\sum_{i = 1}^n x_i\right|. && \text{(since $g(x) \in \{ \pm 1 \}$ for all $x \in \{ \pm 1 \}^n$)}
      \end{align*}

      Third, since $f$ is monotone,
      $$ \Inf(f) = \frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} f(x) \sum_{i = 1}^n x_i. $$
      Since $n$ is odd, then $\sum_{i = 1}^n x_i \neq 0$. If $\sum_{i = 1}^n x_i < 0$, then $f(x) = \sign(\sum_{i = 1}^n x_i) < 0$, so
      \begin{equation} \label{eq:2b-maj}
        \Inf(f) = \frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} |f(x)| \left|\sum_{i = 1}^n x_i\right| = \frac{1}{2^n} \sum_{x = \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n} \left|\sum_{i = 1}^n x_i\right|,
      \end{equation}
      since $f(x) \in \{ \pm 1 \}$ for all $x \in \{ \pm 1 \}^n$. Otherwise, $\sum_{i = 1}^n x_i > 0$, so $f(x) = \sign(\sum_{i = 1}^n x_i) > 0$, implying that \eqref{eq:2b-maj} holds. Hence, \eqref{eq:2b-maj} holds in both cases. It follows that $\Inf(g) \leq \Inf(f)$ for any monotone $g : \{ \pm 1 \}^n \to \{ \pm 1 \}$, completing the proof.
    \end{proof}
  \end{enumerate}

  \clearpage

  \item \begin{enumerate}
    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$. Let $\varepsilon > 0$. We show that the statement holds with $C = 1$. Suppose for the sake of contradiction that $\sum_{\substack{S \subset [n], |S| \geq \Inf(f) / \varepsilon}} \hat{f}(S)^2 > C \varepsilon = \varepsilon$.
      For each $i \in [n]$, let $g_i : \{ \pm 1 \}^n \to \{ 0, \pm 1 \}$ be defined by
      \begin{align*}
        g_i(x) &= \frac{f(x) - f\left(x^{\oplus i}\right)}{2} = \frac{1}{2} \left(\sum_{S \subset [n]} \hat{f}(S) \chi_S(x) - \sum_{S \subset [n]} \hat{f}(S) \chi_S\left(x^{\oplus i}\right)\right) \\
        &= \frac{1}{2} \sum_{S \subset [n]} \hat{f}(S) \left(\chi_S(x) - \chi_S\left(x^{\oplus i}\right)\right).
      \end{align*}
      Then $g_i(x)^2 = \mathds 1[f(x) \neq f(x^{\oplus i})]$ for all $i \in [n]$ and $x \in \{ \pm 1 \}^n$. Fix $i \in [n]$, $x = (x_1, \ldots, x_n) \in \{ \pm 1 \}^n$ and $S \subset [n]$. If $i \in S$, then
      \begin{align*}
        \chi_S(x) - \chi_S\left(x^{\oplus i}\right) &= \prod_{j \in S} x_j - \left(-x_i\right) \prod_{j \in S \setminus \{ i \}} x_j = x_i \prod_{j \in S \setminus \{ i \}} x_j - \left(-x_i\right) \prod_{j \in S \setminus \{ i \}} x_j \\
        &= \left(x_i - \left(-x_i\right)\right) \prod_{j \in S \setminus \{ i \}} x_j = 2x_i \prod_{j \in S \setminus \{ i \}} x_j = 2\prod_{j \in S} x_j = 2\chi_S(x).
      \end{align*}
      If $i \not \in S$, then
      $$ \chi_S(x) - \chi_S\left(x^{\oplus i}\right) = \prod_{j \in S} x_j - \prod_{j \in S} x_j = 0. $$
      Hence, for all $i \in [n]$ and $x \in \{ \pm 1 \}^n$,
      $$ g_i(x) = \frac{1}{2} \sum_{\substack{S \subset [n] \\ i \in S}} \hat{f}(S) \cdot 2\chi_S(x) = \frac{1}{2} \cdot 2 \sum_{\substack{S \subset [n] \\ i \in S}} \hat{f}(S) \chi_S(x) = \sum_{\substack{S \subset [n] \\ i \in S}} \hat{f}(S) \chi_S(x). $$
      For all $i \in [n]$, by the orthonormality of the Fourier basis $\{ \chi_S : S \subset [n] \}$,
      \begin{align*}
        \Inf_i(f) &= \PP_{x \in \{ \pm 1 \}^n}\left[f(x) \neq f\left(x^{\oplus i}\right)\right] = \EE_{x \in \{ \pm 1 \}^n}\left[\mathds 1\left[f(x) \neq f\left(x^{\oplus i}\right)\right]\right] = \EE_{x \in \{ \pm 1 \}^n}\left[g_i(x)^2\right] \\
        &= \EE_{x \in \{ \pm 1 \}^n}\left[\left(\sum_{\substack{S \subset [n] \\ i \in S}} \hat{f}(S) \chi_S(x)\right)^2\right] = \EE_{x \in \{ \pm 1 \}^n}\left[\sum_{\substack{S \subset [n] \\ i \in S}} \sum_{\substack{T \subset [n] \\ i \in T}} \hat{f}(S) \hat{f}(T) \chi_S(x) \chi_T(x)\right] \\
        &= \EE_{x \in \{ \pm 1 \}^n}\left[\sum_{\substack{S \subset [n] \\ i \in S}} \hat{f}(S)^2\right] = \sum_{\substack{S \subset [n] \\ i \in S}} \hat{f}(S)^2.
      \end{align*}
      Therefore,
      \begin{align*}
        \Inf(f) &= \sum_{i = 1}^n \Inf_i(f) = \sum_{i = 1}^n \sum_{\substack{S \subset [n] \\ i \in S}} \hat{f}(S)^2 = \sum_{S \subset [n]} \sum_{i \in S} \hat{f}(S)^2 = \sum_{S \subset [n]} |S| \hat{f}(S)^2 \\
        &\geq \sum_{\substack{S \subset [n] \\ |S| \geq \frac{\Inf(f)}{\varepsilon}}} |S| \hat{f}(S)^2 \geq \frac{\Inf(f)}{\varepsilon} \sum_{\substack{S \subset [n] \\ |S| \geq \frac{\Inf(f)}{\varepsilon}}} \hat{f}(S)^2 > \frac{\Inf(f)}{\varepsilon} \cdot \varepsilon = \Inf(f),
      \end{align*}
      a contradiction. This completes the proof.
    \end{proof}

    \clearpage

    \item \noindent\emph{Collaborators and sources:} Guanghao Ye.
    
    \begin{proof}
      Let $f : \{ \pm 1 \}^n \to \{ \pm 1 \}$ be monotone. Then
      \begin{align*}
        \Inf(f) &= \sum_{i = 1}^n \Inf_i(f) \\
        &= \sum_{i = 1}^n \hat{f}(\{ i \}) && \text{(Problem 2 part (a))} \\
        &\leq \sqrt{n\sum_{i = 1}^n \hat{f}(\{ i \})^2} && \text{(Cauchy-Schwarz inequality)} \\
        &\leq \sqrt{n\sum_{S \subset [n]} \hat{f}(S)^2} \\
        &= \sqrt{n \cdot 1} && \text{(Boolean Parseval's identity)} \\
        &= \sqrt{n}.
      \end{align*}
      By part (a), there exists an absolute constant $C$ such that for all $\varepsilon > 0$,
      $$ \sum_{\substack{S \subset [n] \\ |S| \geq \frac{\sqrt{n}}{\varepsilon}}} \hat{f}(S)^2 \leq \sum_{\substack{S \subset [n] \\ |S| \geq \frac{\Inf(f)}{\varepsilon}}} \hat{f}(S)^2 \leq C\varepsilon. $$
      Hence, any monotone Boolean function has Fourier concentration $\alpha(\varepsilon, n) = C\sqrt{n}/\varepsilon$. The low degree algorithm gives a uniform distribution learning algorithm $\mathcal A$ for the class of monotone Boolean functions with sample complexity
      \begin{align*}
        O\left(\frac{n^{\alpha(\varepsilon, n)}}{\varepsilon} \log \frac{n^{\alpha(\varepsilon, n)}}{\delta}\right) &= O\left(\frac{n^{\alpha(\varepsilon, n)}}{\varepsilon} \cdot \alpha(\varepsilon, n) \log n\right) \\
        &= O\left(\frac{n^{\frac{C\sqrt[]{n}}{\varepsilon}}}{\varepsilon} \cdot \frac{C\sqrt[]{n}}{\varepsilon} \log n\right) \\
        &= O\left(\frac{n^{\frac{C\sqrt[]{n}}{\varepsilon} + \frac{1}{2}}}{\varepsilon^2} \log n\right).
      \end{align*}
      Since $1/\varepsilon^2 \leq (n^{\sqrt{n}})^{1/\varepsilon}$ for sufficiently large $n$ and sufficiently small $\varepsilon$, then the sample complexity of $\mathcal A$ is
      $$ O\left(n^{\frac{C\sqrt[]{n}}{\varepsilon} + \frac{1}{2}} \log n\right) \leq n^{\Theta\left(\frac{\sqrt[]{n}}{\varepsilon}\right)} = \left(2^{\log_2 n}\right)^{\Theta\left(\frac{\sqrt[]{n}}{\varepsilon}\right)} = 2^{(\log_2 n) \Theta\left(\frac{\sqrt[]{n}}{\varepsilon}\right)} = 2^{\Theta\left(\frac{\sqrt[]{n}}{\varepsilon} \log n\right)} = 2^{\widetilde{O}\left(\frac{\sqrt[]{n}}{\varepsilon}\right)}. $$
      This completes the proof.
    \end{proof}
  \end{enumerate}

  \clearpage

  \item \begin{enumerate}
    \item \noindent\emph{Collaborators and sources:} none.
    
    \begin{proof}
      Let $X = (X_1, \ldots, X_n) \in \{ \pm 1 \}^n$ be an $(\varepsilon, k)$-wise independent random vector for some $\varepsilon \in (0, 1)$ and $k \in [n]$. Let $S \subset [n]$ be such that $0 < |S| \leq k$. By straightforward calculations (see, e.g., the proof of Problem 1 part (a) in Homework 2), for all $\ell \in [n]$,
      $$ \PP_{\left(W_1, \ldots, W_\ell\right) \sim \Unif\{ \pm 1 \}^\ell}\left[\prod_{i = 1}^\ell W_\ell = 1\right] = \frac{1}{2}. $$
      Since $X$ is $(\varepsilon, k)$-wise independent and since $0 < |S| \leq k$,
      $$ \left|\PP_X\left[\prod_{i \in S} X_i = 1\right] - \frac{1}{2}\right| = \left|\PP_X\left[\prod_{i \in S} X_i = 1\right] - \PP_{\left(W_1, \ldots, W_\ell\right) \sim \Unif\{ \pm 1 \}^\ell}\left[\prod_{i = 1}^\ell W_\ell = 1\right]\right| \leq \varepsilon. $$
      WLOG, assume that
      $$ \PP_X\left[\prod_{i \in S} X_i = 1\right] = \frac{1 + \varepsilon_0}{2}, $$
      for some $\varepsilon_0 \in [0, 2\varepsilon]$ (the case $\PP_X[\prod_{i \in S} X_i = 1] = (1 - \varepsilon_0)/2$ for some $\varepsilon_0 \in [0, 2\varepsilon]$ is symmetric). Let $\lambda = 1/(1 + \varepsilon_0) \in (0, 1]$. Let $Y = (Y_1, \ldots, Y_n) \in \{ \pm 1 \}^n$ be a random vector defined as follows:
      \begin{enumerate}[label=(\roman*), itemsep=0pt]
        \item With probability $\lambda$, let $Y = X$.
        \item With probability $1 - \lambda$, let $Y$ be uniform over
        $$ \mathcal W := \left\{ \left(x_1, \ldots, x_n\right) \in \{ \pm 1 \}^n : \prod_{i \in S} x_i = -1 \right\}. $$
      \end{enumerate}
      Then
      $$ \PP_Y\left[\prod_{i \in S} Y_i = 1\right] = \lambda \PP_X\left[\prod_{i \in S} X_i = 1\right] + (1 - \lambda) \cdot 0 = \frac{1}{1 + \varepsilon_0} \cdot \frac{1 + \varepsilon_0}{2} = \frac{1}{2}. $$
      For all $\mathcal T \subset \{ \pm 1 \}^n$,
      \begin{align*}
        \PP_X[X \in \mathcal T] - \PP_Y[Y \in \mathcal T] &= \PP_X[X \in \mathcal T] - \left(\lambda \PP_X[X \in \mathcal T] + (1 - \lambda) \PP_{W \sim \Unif \mathcal W}[W \in \mathcal T]\right) \\
        &= (1 - \lambda) \left(\PP_X[X \in \mathcal T] - \PP_{W \sim \Unif \mathcal W}[W \in \mathcal T]\right) \\
        &\leq (1 - \lambda) (1 - 0) = 1 - \frac{1}{1 + \varepsilon_0} = \frac{\varepsilon_0}{1 + \varepsilon_0} \\
        &\leq \varepsilon_0 \leq 2\varepsilon.
      \end{align*}
      Therefore,
      $$ \Delta(X, Y) = \max_{\mathcal T \subset \{ \pm 1 \}^n} \left(\PP_X[X \in \mathcal T] - \PP_Y[Y \in \mathcal T]\right) \leq 2\varepsilon. $$
      This completes the proof.
    \end{proof}
  \end{enumerate}
\end{enumerate}

\end{document}
